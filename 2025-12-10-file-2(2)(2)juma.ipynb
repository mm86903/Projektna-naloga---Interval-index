{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1fb7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete\n"
     ]
    }
   ],
   "source": [
    "# load previously generated graphs\n",
    "import csv\n",
    "\n",
    "def load_results(filename):\n",
    "    results = []   # list of (Int(G), Radius)\n",
    "\n",
    "    with open(filename, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            IntG   = int(row[\"Ind(G)\"])\n",
    "            Radius = int(row[\"Radius\"])\n",
    "            results.append((IntG, Radius))\n",
    "\n",
    "    return results\n",
    "\n",
    "n=18\n",
    "\n",
    "results = load_results(f\"C{n}.csv\")\n",
    "\n",
    "all_cubic  = load(\"C18_cubic_graphs_n10.sobj\")\n",
    "print(\"Load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a4ec5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
   ],
   "source": [
    "def interval_index(G):\n",
    "    V = G.vertices()  # dobimo vsa vozlišča\n",
    "    Int_G = 0\n",
    "    \n",
    "    # Iteriramo cez  vse pare vozlisc {u, v}\n",
    "    for u in V:\n",
    "        for v in V:\n",
    "            # Izognemo se dvojnemu stetju (u,v) in (v,u) in (u,u)!\n",
    "            if u >= v:\n",
    "                continue\n",
    "            d_uv = G.distance(u, v)    # poiscemo razdaljo\n",
    "            \n",
    "            interval_size = 0   # stevec za |I(u,v)|\n",
    "            \n",
    "            for w in V:\n",
    "                d_uw = G.distance(u, w)\n",
    "                d_wv = G.distance(w, v)\n",
    "                \n",
    "                if d_uw + d_wv == d_uv:\n",
    "                    interval_size += 1\n",
    "            \n",
    "            Int_G += (interval_size - 1)\n",
    "            \n",
    "    return Int_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c678",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File savedC18_cubic_graphs_n10.sobj\n"
     ]
    }
   ],
   "source": [
    "from sage.all import *\n",
    "\n",
    "deg = 3\n",
    "n = 18\n",
    "\n",
    "all_cubic = list(graphs.nauty_geng(f\"{n} -c -d3 -D3\"))\n",
    "\n",
    "save(all_cubic, f\"C{n}_cubic_graphs_n10.sobj\")\n",
    "print(f\"File savedC{n}_cubic_graphs_n10.sobj\")\n",
    "\n",
    "results = []\n",
    "for G in all_cubic:\n",
    "    if G.order() == n and G.is_regular(3):  # Verify cubic\n",
    "        func_val = interval_index(G)\n",
    "        diam = G.diameter()\n",
    "        results.append([func_val, diam])\n",
    "\n",
    "num_graphs = len(results)\n",
    "print(f\"There are {num_graphs} non isomorphic cubic graphs on {n} vertices.\")\n",
    "print(f\"Number of vertices: {n}\")\n",
    "print(f\"Number of generated graphs: {num_graphs}\")\n",
    "\n",
    "Gpts = point(results, color = 'lightblue' , size=20)\n",
    "labels = sum(\n",
    "    text(str(i+1), (results[i][0], results[i][1]), color='black', fontsize=5)\n",
    "    for i in range(len(results))\n",
    ")\n",
    "\n",
    "p = Gpts + labels\n",
    "p.axes_labels(['Int(G)', 'diameter'])\n",
    "p.show() \n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "# Plot points\n",
    "#points = point(results, rgbcolor=(0,0,1), size=50, title=f\"Cubic graphs on {n} vertices: Int(G) vs diameter\")\n",
    "#points.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c47",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**N = 4**\n",
    "\n",
    "Obstaja en neizomorfen kubičen graf G na 4 vozliščih.\n",
    "\n",
    "Int\\(G\\) = 6, radij grafa je 1.\n",
    "\n",
    "Int\\(pot4\\) = 10.\n",
    "\n",
    "**N = 6**\n",
    "\n",
    "Obstajata dva neizomorfna kubična gafa na 6 vozliščih.\n",
    "\n",
    "Oba imasta radij 2, Int\\(G\\) grafov je 27 in 33. Večji Int\\(G\\) ima graf, ki je bipartiten in ima večji Aut\\(G\\).\n",
    "\n",
    "Int\\(pot6\\) = 35.\n",
    "\n",
    "**N = 8**\n",
    "\n",
    "Obstaja 5 neizomorfnih kubičnih gafov G na 8 vozliščih.\n",
    "\n",
    "Dva grafa imasta radij 2, prvi ima Int\\(G\\) = 49, drugi pa Int\\(G\\) = 52.\n",
    "\n",
    "Trije graf imajo radij 3. Int\\(G\\) pa 58, 68 in 76.\n",
    "\n",
    "Edini največji Int\\(G\\) ima edini bipartitni graf, ki ima tudi največji Aut\\(G\\). \n",
    "\n",
    "Ostali grafi niso bipartitni in imajo večji Int\\(G\\) pri večjem Aut\\(G\\).\n",
    "\n",
    "In imajo pri istem diametru večji Int\\(G\\) in Aut\\(G\\).\n",
    "\n",
    "**N = 10**\n",
    "\n",
    "Obstaja 19 neizomorfnih kubičnih gafov na 10 vozliščih. Dva sta bipartitna, vsi ostali pa ne.\n",
    "\n",
    "Int\\(pot10\\) = 165.\n",
    "\n",
    "Graf z najmanjšim Int\\(G\\) ima največji Aut\\(G\\) in hkrati najmanši radij.\n",
    "\n",
    "Edini, najvišji radij 5, ima graf z najvišjim Int\\(G\\). \n",
    "\n",
    "Večina \\(80%\\) grafov ima radij 3.\n",
    "\n",
    "**N = 12**\n",
    "\n",
    "Obstaja 85 neizomorfnih kubičnih grafov na 12 vozliščih.\n",
    "\n",
    "Int\\(pot12\\) = 286.\n",
    "\n",
    "Radij grafov sega od 3 pa do 6. Večina ima radij ali 3 ali 4. Od 85 grafov imata dva radij 6, pet jih ima radij 5.\n",
    "\n",
    "Skoraj noben graf ni bipartiten.\n",
    "\n",
    "Int\\(G\\) pada, ko pada tudi radij.\n",
    "\n",
    "Pri n=12 ima bipartitnost večji vpil kot globina grafa. \n",
    "\n",
    "**N =14**\n",
    "\n",
    "Obstaja 509 neizomorfnih kubičnih grafov na 14 vozliščih.\n",
    "\n",
    "Int\\(pot14\\) = 455.\n",
    "\n",
    "Z večjim radijem, se lepo veča tudi indeks.\n",
    "\n",
    "Prileganje radija in iintervalnega ndeksa je zelo naravno. Večji radij, da višji indeks.\n",
    "\n",
    "Najpogostejši radij je 4. Odstopanja niso velika.\n",
    "\n",
    "Bipartitni grafi so zelo redki, imajo pa najvišji Int\\(G\\).\n",
    "\n",
    "**N = 16**\n",
    "\n",
    "Obstaja 4060 neizomorfnih kubičnih grafov na 16 vozliščih.\n",
    "\n",
    "Int\\(pot16\\) = 680.\n",
    "\n",
    "Prileganje radija in iintervalnega ndeksa je zelo naravno. Stopničasto. Zelo kolerirano.\n",
    "\n",
    "Vplivnost radija \\(78%\\) je zelo visoka in vpliv bipartitnosti \\(10%\\). \n",
    "\n",
    "Int\\(G\\) na kubičnih grafih na 16 vozliščih je najvrjetneje 4 ali 5 ali 6 ai 7.\n",
    "\n",
    "In 3 in 8 in 9 so pa preostali, manj pogosti Int\\(G\\).\n",
    "\n",
    "**N = 18**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "ad7cf7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14ae4d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
   ],
   "source": [
    "m = len(all_cubic)                # number of graphs\n",
    "\n",
    "rows = []\n",
    "for i, G in enumerate(all_cubic):\n",
    "    if i < len(results):\n",
    "        interval, radius = results[i]\n",
    "    else:\n",
    "        interval, radius = \"?\", \"??\"\n",
    "\n",
    "    # per‑graph size; kept constant\n",
    "    P = G.plot(graph_border=False,\n",
    "               vertex_size=200)   # adjust once for readability [web:11]\n",
    "\n",
    "    lbl = text(f\"Ind: {i+1}  Int(G): {interval}  Radius: {radius}\",\n",
    "               (0, -0.3), fontsize=10)\n",
    "\n",
    "    rows.append(P + lbl)\n",
    "\n",
    "GA = graphics_array(rows, nrows=m, ncols=1)\n",
    "\n",
    "# overall figure height grows with m\n",
    "base_height = 3                   # height per graph+label row\n",
    "GA.save(f\"C{n}_all_graphs.pdf\",\n",
    "        axes=False,\n",
    "        figsize=[8, base_height*m])   # e.g. 5 graphs → height 15 [web:23][web:60]\n",
    "print(f\"File C{n}_all_graphs.pdf generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "da8020",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
   ],
   "source": [
    "# Choose a file name; it will appear in the same directory as your worksheet/notebook\n",
    "import csv  \n",
    "\n",
    "filename = f\"C{n}.csv\"\n",
    "\n",
    "with open(filename, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"I\",\"Ind(G)\",\"Radius\"])\n",
    "    # If A is a Sage/Python list of lists or a Sage matrix, this works:\n",
    "    i=1\n",
    "    for row in results:\n",
    "        writer.writerow([i]+list(row))\n",
    "        i +=1\n",
    "\n",
    "print(\"Saved to\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c31f88",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int(PathGraph(16) = 680.\n"
     ]
    }
   ],
   "source": [
    "vert = interval_index(graphs.PathGraph(n))\n",
    "print(f\"Int(PathGraph({n}) = {vert}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71140b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function numerical_approx at 0x7f5c5e2c4900>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ind  \u001b[38;5;241m=\u001b[39m Integer(\u001b[38;5;241m19\u001b[39m)\n\u001b[1;32m      3\u001b[0m ind \u001b[38;5;241m=\u001b[39m ind\u001b[38;5;241m-\u001b[39mInteger(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresults\u001b[49m[ind])\n\u001b[1;32m      5\u001b[0m all_cubic[ind]\u001b[38;5;241m.\u001b[39mplot()\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(n)\n",
    "ind\n",
    "ind = ind-1\n",
    "print(results[ind])\n",
    "all_cubic[ind].plot().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18c02e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV written to C16_cubic_graphs.csv\n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "from itertools import combinations\n",
    "\n",
    "########################################\n",
    "# 1) Interval-based indices\n",
    "########################################\n",
    "\n",
    "def interval_size(G, u, v):\n",
    "    du = G.distances(source=u)\n",
    "    dv = G.distances(source=v)\n",
    "    d_uv = du[v]\n",
    "    return sum(1 for w in G.vertices() if du[w] + dv[w] == d_uv)\n",
    "\n",
    "def Int_index(G):\n",
    "    dist = G.distance_all_pairs()   # dict-of-dicts\n",
    "    V = G.vertices()\n",
    "    s = 0\n",
    "    for u, v in combinations(V, 2):\n",
    "        d_uv = dist[u][v]\n",
    "        # count vertices on some shortest u-v path\n",
    "        cnt = sum(1 for w in V if dist[u][w] + dist[w][v] == d_uv)\n",
    "        s += cnt - 1\n",
    "    return s\n",
    "\n",
    "def wiener_index(G):\n",
    "    dist = G.distance_all_pairs()\n",
    "    return sum(dist[u][v] for u, v in combinations(G.vertices(), 2))\n",
    "\n",
    "########################################\n",
    "# 2) Graph classification / properties\n",
    "########################################\n",
    "\n",
    "def graph_data(G, graph_id):\n",
    "    IntG = Int_index(G)\n",
    "    WG   = wiener_index(G)\n",
    "\n",
    "    return {\n",
    "        \"id\": graph_id,\n",
    "        \"n\": G.order(),\n",
    "        \"m\": G.size(),\n",
    "        \"planar\": G.is_planar(),\n",
    "        \"bipartite\": G.is_bipartite(),\n",
    "        \"bridgeless\": (G.edge_connectivity() >= 2),\n",
    "        \"girth\": G.girth(),\n",
    "        \"diameter\": G.diameter(),\n",
    "        \"edge_connectivity\": G.edge_connectivity(),\n",
    "        \"vertex_connectivity\": G.vertex_connectivity(),\n",
    "        \"automorphism_group_order\": G.automorphism_group().order(),\n",
    "        \"chromatic_index\": G.chromatic_index(),\n",
    "        \"Int(G)\": IntG,\n",
    "        \"Wiener(G)\": WG,\n",
    "        \"Int(G)-Wiener(G)\": IntG - WG\n",
    "    }\n",
    "\n",
    "########################################\n",
    "# 3) Generate cubic graphs\n",
    "########################################\n",
    "\n",
    "graphs_n = all_cubic\n",
    "\n",
    "\n",
    "########################################\n",
    "# 4) Write CSV\n",
    "########################################\n",
    "\n",
    "output_file = f\"C{n}_cubic_graphs.csv\"\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = None\n",
    "\n",
    "    for i, G in enumerate(graphs_n):\n",
    "        row = graph_data(G, i)\n",
    "\n",
    "        if writer is None:\n",
    "            # first row = column names\n",
    "            writer = csv.DictWriter(f, fieldnames=row.keys())\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV written to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0422",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C<function numerical_approx at 0x7f5c5e2c4900>_cubic_graphs.csv\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C<function numerical_approx at 0x7f5c5e2c4900>_cubic_graphs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 118\u001b[0m\n\u001b[1;32m    116\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cubic_graphs.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(csv_file)\n\u001b[0;32m--> 118\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mrf_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInt(G)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRealNumber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m0.25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mInteger\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m, in \u001b[0;36mrf_analysis\u001b[0;34m(csv_file, target_col, test_size, random_state, n_estimators, n_repeats)\u001b[0m\n\u001b[1;32m     59\u001b[0m n_estimators_py \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_estimators)\n\u001b[1;32m     60\u001b[0m n_repeats_py    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_repeats)\n\u001b[0;32m---> 62\u001b[0m X, y, feature_names \u001b[38;5;241m=\u001b[39m \u001b[43mload_xy_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile loaded: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mcsv_file)\n\u001b[1;32m     65\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     66\u001b[0m     X, y, test_size\u001b[38;5;241m=\u001b[39mtest_size_py, random_state\u001b[38;5;241m=\u001b[39mrandom_state_py\n\u001b[1;32m     67\u001b[0m )\n",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m, in \u001b[0;36mload_xy_from_csv\u001b[0;34m(filename, target_col, drop_cols)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_xy_from_csv\u001b[39m(filename, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInt(G)\u001b[39m\u001b[38;5;124m\"\u001b[39m, drop_cols\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m,)):\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     28\u001b[0m         reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictReader(f)\n\u001b[1;32m     29\u001b[0m         cols \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mfieldnames\n",
      "File \u001b[0;32m/ext/sage/10.7/local/var/lib/sage/venv-python3.12.5/lib/python3.12/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C<function numerical_approx at 0x7f5c5e2c4900>_cubic_graphs.csv'"
     ]
    }
   ],
   "source": [
    "#From these results you can already justify the statement:\n",
    "#For cubic graphs, the interval index Int(G) is overwhelmingly determined by global distance structure (Wiener index), while classical structural properties (planarity, girth, connectivity, coloring) have negligible explanatory power.\n",
    "#That is a nontrivial structural insight, and it aligns perfectly with metric graph theory.\n",
    "#\n",
    "# However: \n",
    "# The key issue: target leakage \n",
    "# We are predicting Int(G), and feature list includes:\n",
    "#Wiener(G)\n",
    "#Int(G)-Wiener(G)\n",
    "#But mathematically:\n",
    "#Int(G)=Wiener(G)+(Int(G)−Wiener(G))\n",
    "#So the model is being given two parts whose sum is exactly the target.\n",
    "#Consequence\n",
    "#This is perfect information leakage.\n",
    "#The Random Forest is not “discovering” a relationship — it is simply recombining components of provided Int(G) .\n",
    "#R² ≈ 1\n",
    "#huge importance for Wiener(G) and Int(G)-Wiener(G)\n",
    "#everything else being negligible\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def load_xy_from_csv(filename, target_col=\"Int(G)\", drop_cols=(\"id\",)):\n",
    "    with open(filename, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        if cols is None:\n",
    "            raise ValueError(\"CSV has no header row.\")\n",
    "\n",
    "        drop = set(drop_cols)\n",
    "        if target_col not in cols:\n",
    "            raise KeyError(f\"Target column '{target_col}' not found. Available: {cols}\")\n",
    "\n",
    "        feature_names = [c for c in cols if (c != target_col and c not in drop)]\n",
    "\n",
    "        X_rows, y = [], []\n",
    "        for row in reader:\n",
    "            y.append(float(row[target_col]))\n",
    "            feat = []\n",
    "            for c in feature_names:\n",
    "                val = row[c]\n",
    "                if val in (\"True\", \"False\"):\n",
    "                    feat.append(1.0 if val == \"True\" else 0.0)\n",
    "                else:\n",
    "                    feat.append(float(val))\n",
    "            X_rows.append(feat)\n",
    "\n",
    "    return np.array(X_rows, dtype=float), np.array(y, dtype=float), feature_names\n",
    "\n",
    "def rf_analysis(csv_file, target_col=\"Int(G)\", test_size=0.25, random_state=0,\n",
    "                n_estimators=200, n_repeats=5):\n",
    "\n",
    "    # Force sklearn-friendly base Python types (important in Sage)\n",
    "    test_size_py   = float(test_size) if test_size is not None else None\n",
    "    random_state_py = int(random_state) if random_state is not None else None\n",
    "    n_estimators_py = int(n_estimators)\n",
    "    n_repeats_py    = int(n_repeats)\n",
    "\n",
    "    X, y, feature_names = load_xy_from_csv(csv_file, target_col=target_col, drop_cols=(\"id\",))\n",
    "    print(\"File loaded: \"+csv_file)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size_py, random_state=random_state_py\n",
    "    )\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=n_estimators_py,\n",
    "        random_state=random_state_py,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    r2_train = rf.score(X_train, y_train)\n",
    "    r2_test  = rf.score(X_test, y_test)\n",
    "\n",
    "    imp = rf.feature_importances_\n",
    "\n",
    "    perm = permutation_importance(\n",
    "        rf, X_test, y_test,\n",
    "        n_repeats=n_repeats_py,\n",
    "        random_state=random_state_py,\n",
    "        n_jobs=1\n",
    "    )\n",
    "\n",
    "    perm_mean = perm.importances_mean\n",
    "    perm_std  = perm.importances_std\n",
    "\n",
    "    print(\"=== Random Forest results ===\")\n",
    "    print(f\"Samples: {len(y)} | Features: {X.shape[1]}\")\n",
    "    print(f\"R^2 train: {r2_train:.4f}\")\n",
    "    print(f\"R^2 test : {r2_test:.4f}\\n\")\n",
    "\n",
    "    print(\"Top features (impurity importance):\")\n",
    "    order = np.argsort(imp)[::-1]\n",
    "    for k in order[:15]:\n",
    "        print(f\"  {feature_names[k]:30s}  {imp[k]:.6f}\")\n",
    "\n",
    "    print(\"\\nTop features (permutation importance on test set):\")\n",
    "    order2 = np.argsort(perm_mean)[::-1]\n",
    "    for k in order2[:15]:\n",
    "        print(f\"  {feature_names[k]:30s}  {perm_mean[k]:.6f}  +/- {perm_std[k]:.6f}\")\n",
    "\n",
    "    return {\n",
    "        \"rf\": rf,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "        \"impurity_importance\": imp,\n",
    "        \"perm_mean\": perm_mean,\n",
    "        \"perm_std\": perm_std,\n",
    "    }\n",
    "\n",
    "csv_file = f\"C{n}_cubic_graphs.csv\"\n",
    "print(csv_file)\n",
    "out = rf_analysis(csv_file, target_col=\"Int(G)\", test_size=0.25, random_state=0)\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05c26c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest (LEAKAGE REMOVED) ===\n",
      "File: C16_cubic_graphs.csv\n",
      "Target: Int(G)\n",
      "Removed leakage cols from features: ['Wiener(G)', 'Int(G)-Wiener(G)']\n",
      "Samples: 4060 | Features used: 11\n",
      "R^2 train: 0.6395\n",
      "R^2 test : 0.6368\n",
      "\n",
      "Top features (impurity importance):\n",
      "  diameter                        0.785732\n",
      "  bipartite                       0.103501\n",
      "  automorphism_group_order        0.068911\n",
      "  girth                           0.016089\n",
      "  planar                          0.012866\n",
      "  vertex_connectivity             0.004377\n",
      "  edge_connectivity               0.004080\n",
      "  chromatic_index                 0.002909\n",
      "  bridgeless                      0.001535\n",
      "  m                               0.000000\n",
      "  n                               0.000000\n",
      "\n",
      "Top features (permutation importance on test set):\n",
      "  diameter                        0.992671  +/- 0.036086\n",
      "  bipartite                       0.172839  +/- 0.008747\n",
      "  automorphism_group_order        0.050678  +/- 0.010200\n",
      "  girth                           0.006929  +/- 0.003389\n",
      "  chromatic_index                 0.001208  +/- 0.001664\n",
      "  edge_connectivity               0.000395  +/- 0.001430\n",
      "  bridgeless                      0.000205  +/- 0.000461\n",
      "  n                               0.000000  +/- 0.000000\n",
      "  m                               0.000000  +/- 0.000000\n",
      "  vertex_connectivity             -0.000024  +/- 0.001501\n",
      "  planar                          -0.001218  +/- 0.005577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest (LEAKAGE REMOVED) ===\n",
      "File: C16_cubic_graphs.csv\n",
      "Target: Int(G)-Wiener(G)\n",
      "Removed leakage cols from features: ['Int(G)', 'Wiener(G)']\n",
      "Samples: 4060 | Features used: 11\n",
      "R^2 train: 0.3155\n",
      "R^2 test : 0.3018\n",
      "\n",
      "Top features (impurity importance):\n",
      "  bipartite                       0.483654\n",
      "  diameter                        0.159673\n",
      "  automorphism_group_order        0.151011\n",
      "  girth                           0.056267\n",
      "  planar                          0.047816\n",
      "  vertex_connectivity             0.038299\n",
      "  edge_connectivity               0.034917\n",
      "  chromatic_index                 0.018086\n",
      "  bridgeless                      0.010276\n",
      "  m                               0.000000\n",
      "  n                               0.000000\n",
      "\n",
      "Top features (permutation importance on test set):\n",
      "  bipartite                       0.403956  +/- 0.015716\n",
      "  diameter                        0.254159  +/- 0.026117\n",
      "  automorphism_group_order        0.034233  +/- 0.010907\n",
      "  girth                           0.031499  +/- 0.010156\n",
      "  vertex_connectivity             0.021877  +/- 0.007023\n",
      "  edge_connectivity               0.019057  +/- 0.006904\n",
      "  chromatic_index                 0.010768  +/- 0.003923\n",
      "  planar                          0.005516  +/- 0.012318\n",
      "  bridgeless                      0.001631  +/- 0.002155\n",
      "  m                               0.000000  +/- 0.000000\n",
      "  n                               0.000000  +/- 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Leakage-free Random Forest analysis for Int(G) (Sage-friendly)\n",
    "# - removes \"Wiener(G)\" and \"Int(G)-Wiener(G)\" from features by default\n",
    "# - also supports predicting the residual Int(G)-Wiener(G) as a separate target\n",
    "\n",
    "import os\n",
    "# (recommended) limit native threading to avoid kernel crashes\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "def load_xy_from_csv(\n",
    "    filename,\n",
    "    target_col=\"Int(G)\",\n",
    "    drop_cols=(\"id\",),\n",
    "    drop_feature_cols=(),\n",
    "):\n",
    "    \"\"\"\n",
    "    Load CSV -> (X, y, feature_names)\n",
    "\n",
    "    - target_col: column to predict\n",
    "    - drop_cols: columns never used (e.g. id)\n",
    "    - drop_feature_cols: columns explicitly excluded from FEATURES (e.g. leakage)\n",
    "    \"\"\"\n",
    "    with open(filename, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        cols = reader.fieldnames\n",
    "        if cols is None:\n",
    "            raise ValueError(\"CSV has no header row.\")\n",
    "        if target_col not in cols:\n",
    "            raise KeyError(f\"Target column '{target_col}' not found. Available: {cols}\")\n",
    "\n",
    "        drop_cols = set(drop_cols)\n",
    "        drop_feature_cols = set(drop_feature_cols)\n",
    "\n",
    "        feature_names = [\n",
    "            c for c in cols\n",
    "            if (c != target_col) and (c not in drop_cols) and (c not in drop_feature_cols)\n",
    "        ]\n",
    "\n",
    "        X_rows, y = [], []\n",
    "        for row in reader:\n",
    "            # target\n",
    "            y.append(float(row[target_col]))\n",
    "\n",
    "            # features\n",
    "            feat = []\n",
    "            for c in feature_names:\n",
    "                val = row[c]\n",
    "                if val in (\"True\", \"False\"):\n",
    "                    feat.append(1.0 if val == \"True\" else 0.0)\n",
    "                else:\n",
    "                    # handle Sage \"Infinity\" if it ever appears\n",
    "                    if val in (\"+Infinity\", \"Infinity\", \"inf\", \"Inf\"):\n",
    "                        feat.append(float(\"inf\"))\n",
    "                    else:\n",
    "                        feat.append(float(val))\n",
    "            X_rows.append(feat)\n",
    "\n",
    "    X = np.array(X_rows, dtype=float)\n",
    "    y = np.array(y, dtype=float)\n",
    "\n",
    "    # If any inf values exist, you must decide how to handle them.\n",
    "    # For cubic connected graphs, girth/diameter/etc should be finite.\n",
    "    if not np.isfinite(X).all():\n",
    "        raise ValueError(\"Non-finite values (inf/nan) found in features. Clean your CSV or drop those columns.\")\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "\n",
    "def rf_analysis_no_leakage(\n",
    "    csv_file,\n",
    "    target_col=\"Int(G)\",\n",
    "    leakage_cols=(\"Wiener(G)\", \"Int(G)-Wiener(G)\"),\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    n_estimators=300,\n",
    "    n_repeats=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Random Forest regression with leakage columns removed from features.\n",
    "\n",
    "    Returns a dict with model + importances.\n",
    "    \"\"\"\n",
    "    # Force sklearn-friendly base Python types (important in Sage)\n",
    "    test_size_py = float(test_size) if test_size is not None else None\n",
    "    rs_py = int(random_state) if random_state is not None else None\n",
    "\n",
    "    X, y, feature_names = load_xy_from_csv(\n",
    "        csv_file,\n",
    "        target_col=target_col,\n",
    "        drop_cols=(\"id\",),\n",
    "        drop_feature_cols=leakage_cols,   # <-- leakage removed here\n",
    "    )\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size_py, random_state=rs_py\n",
    "    )\n",
    "\n",
    "    # n_jobs=1 is safest in Sage/Jupyter\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=int(n_estimators),\n",
    "        random_state=rs_py,\n",
    "        n_jobs=1,\n",
    "    )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    r2_train = rf.score(X_train, y_train)\n",
    "    r2_test  = rf.score(X_test, y_test)\n",
    "\n",
    "    # Permutation importance (most meaningful)\n",
    "    perm = permutation_importance(\n",
    "        rf, X_test, y_test,\n",
    "        n_repeats=int(n_repeats),\n",
    "        random_state=rs_py,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    perm_mean = perm.importances_mean\n",
    "    perm_std  = perm.importances_std\n",
    "\n",
    "    # Impurity-based importance (can be biased, still useful)\n",
    "    imp = rf.feature_importances_\n",
    "\n",
    "    print(\"=== Random Forest (LEAKAGE REMOVED) ===\")\n",
    "    print(f\"File: {csv_file}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Removed leakage cols from features: {list(leakage_cols)}\")\n",
    "    print(f\"Samples: {len(y)} | Features used: {len(feature_names)}\")\n",
    "    print(f\"R^2 train: {r2_train:.4f}\")\n",
    "    print(f\"R^2 test : {r2_test:.4f}\\n\")\n",
    "\n",
    "    order_imp = np.argsort(imp)[::-1]\n",
    "    print(\"Top features (impurity importance):\")\n",
    "    for k in order_imp[:15]:\n",
    "        print(f\"  {feature_names[k]:30s}  {imp[k]:.6f}\")\n",
    "\n",
    "    order_perm = np.argsort(perm_mean)[::-1]\n",
    "    print(\"\\nTop features (permutation importance on test set):\")\n",
    "    for k in order_perm[:15]:\n",
    "        print(f\"  {feature_names[k]:30s}  {perm_mean[k]:.6f}  +/- {perm_std[k]:.6f}\")\n",
    "\n",
    "    return {\n",
    "        \"rf\": rf,\n",
    "        \"feature_names\": feature_names,\n",
    "        \"r2_train\": r2_train,\n",
    "        \"r2_test\": r2_test,\n",
    "        \"impurity_importance\": imp,\n",
    "        \"perm_mean\": perm_mean,\n",
    "        \"perm_std\": perm_std,\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# USAGE EXAMPLES\n",
    "# -----------------------------\n",
    "\n",
    "n = 16\n",
    "csv_file = f\"C{n}_cubic_graphs.csv\"\n",
    "\n",
    "# A) Predict Int(G) using ONLY non-leakage properties\n",
    "out_int = rf_analysis_no_leakage(\n",
    "    csv_file,\n",
    "    target_col=\"Int(G)\",\n",
    "    leakage_cols=(\"Wiener(G)\", \"Int(G)-Wiener(G)\"),\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    n_estimators=300,\n",
    "    n_repeats=10\n",
    ")\n",
    "\n",
    "# B) (Recommended) Predict the \"extra geodesic mass\" Int(G)-Wiener(G)\n",
    "#    using classical invariants (this is often more interesting scientifically)\n",
    "out_extra = rf_analysis_no_leakage(\n",
    "    csv_file,\n",
    "    target_col=\"Int(G)-Wiener(G)\",\n",
    "    leakage_cols=(\"Int(G)\", \"Wiener(G)\"),  # remove parts that trivially reconstruct the target\n",
    "    test_size=0.25,\n",
    "    random_state=0,\n",
    "    n_estimators=300,\n",
    "    n_repeats=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd1a26",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c3ac0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] C6_cubic_graphs.csv: 2 rows\n",
      "[load] C8_cubic_graphs.csv: 5 rows\n",
      "[load] C10_cubic_graphs.csv: 19 rows\n",
      "[load] C12_cubic_graphs.csv: 85 rows\n",
      "[load] C14_cubic_graphs.csv: 509 rows\n",
      "[skip] missing file: C16_cubic_graphs.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load] C18_cubic_graphs.csv: 33561 rows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Split: train on n in [6,18]\\{18}, test on n=18 ===\n",
      "Samples total: 34181 | train: 620 | test: 33561\n",
      "Features used: 12\n",
      "Target: Int(G) (normalized by C(n,2))\n",
      "Leakage removed from features: ['Wiener(G)', 'Int(G)-Wiener(G)']\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest (combined n, leakage removed) ===\n",
      "R^2 train: 0.7909\n",
      "R^2 test : -0.8123\n",
      "\n",
      "Top features (impurity importance):\n",
      "  diameter                       0.662466\n",
      "  bipartite                      0.152152\n",
      "  automorphism_group_order       0.088702\n",
      "  n_from_filename                0.015555\n",
      "  m                              0.015499\n",
      "  n                              0.014964\n",
      "  girth                          0.014547\n",
      "  planar                         0.014361\n",
      "  edge_connectivity              0.007928\n",
      "  vertex_connectivity            0.007768\n",
      "  chromatic_index                0.004302\n",
      "  bridgeless                     0.001757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top features (permutation importance on test set):\n",
      "  diameter                       0.993061 +/- 0.007135\n",
      "  bipartite                      0.040436 +/- 0.000780\n",
      "  chromatic_index                0.003520 +/- 0.000249\n",
      "  bridgeless                     0.001274 +/- 0.000134\n",
      "  n                              0.000000 +/- 0.000000\n",
      "  m                              0.000000 +/- 0.000000\n",
      "  n_from_filename                0.000000 +/- 0.000000\n",
      "  automorphism_group_order       -0.029744 +/- 0.001759\n",
      "  planar                         -0.032315 +/- 0.000742\n",
      "  vertex_connectivity            -0.033202 +/- 0.000486\n",
      "  edge_connectivity              -0.035491 +/- 0.000510\n",
      "  girth                          -0.051656 +/- 0.000987\n",
      "\n",
      "Permutation importance of n_from_filename: 0.000000 +/- 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Combined (multi-n) leakage-free analysis for Int(G) from CSVs:\n",
    "#   files: C{n}_cubic_graphs.csv for even n in [nMin, nMax]\n",
    "# Runs RandomForest with:\n",
    "#   - leakage columns removed from FEATURES\n",
    "#   - optional normalization of target to reduce size-dominance\n",
    "#   - option to test on an unseen n (recommended)\n",
    "\n",
    "import os\n",
    "# Prevent kernel crashes from OpenMP/BLAS oversubscription (set BEFORE numpy/sklearn import)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import csv\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# User settings\n",
    "# -----------------------------\n",
    "nMin = 6\n",
    "nMax = 18\n",
    "\n",
    "# Choose ONE target mode:\n",
    "TARGET_COL = \"Int(G)\"          # raw target from CSV\n",
    "NORMALIZE_TARGET = True        # if True: predict Int(G)/C(n,2) instead of Int(G)\n",
    "\n",
    "# Leakage removal (for predicting Int(G) fairly)\n",
    "LEAKAGE_COLS = (\"Wiener(G)\", \"Int(G)-Wiener(G)\")\n",
    "\n",
    "# Split mode:\n",
    "TEST_ON_UNSEEN_N = True        # recommended: hold out one n completely\n",
    "HELD_OUT_N = nMax              # which n to hold out if TEST_ON_UNSEEN_N=True\n",
    "\n",
    "# RF parameters (safe in notebooks)\n",
    "N_ESTIMATORS = 300\n",
    "N_REPEATS_PI = 10\n",
    "RANDOM_STATE = 0\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def choose2(n: int) -> int:\n",
    "    return n * (n - 1) // 2\n",
    "\n",
    "def file_for_n(n: int) -> str:\n",
    "    return f\"C{n}_cubic_graphs.csv\"\n",
    "\n",
    "def read_rows_with_n(filename: str, n_value: int):\n",
    "    \"\"\"Read CSV rows and add an explicit 'n_from_filename'.\"\"\"\n",
    "    with open(filename, newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        if reader.fieldnames is None:\n",
    "            raise ValueError(f\"{filename}: missing header row\")\n",
    "        rows = []\n",
    "        for r in reader:\n",
    "            r[\"n_from_filename\"] = str(n_value)\n",
    "            rows.append(r)\n",
    "        return reader.fieldnames + [\"n_from_filename\"], rows\n",
    "\n",
    "def load_combined_data(nMin: int, nMax: int):\n",
    "    \"\"\"\n",
    "    Loads all available C{n}_cubic_graphs.csv (even n).\n",
    "    Returns (all_rows, all_columns).\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    all_cols = None\n",
    "\n",
    "    for n in range(nMin, nMax + 1, 2):\n",
    "        fn = file_for_n(n)\n",
    "        if not os.path.exists(fn):\n",
    "            print(f\"[skip] missing file: {fn}\")\n",
    "            continue\n",
    "\n",
    "        cols, rows = read_rows_with_n(fn, n)\n",
    "        print(f\"[load] {fn}: {len(rows)} rows\")\n",
    "\n",
    "        # Ensure consistent columns across files\n",
    "        if all_cols is None:\n",
    "            all_cols = cols\n",
    "        else:\n",
    "            # Require same base columns (order can differ; DictReader uses names)\n",
    "            missing = set(all_cols) - set(cols)\n",
    "            extra = set(cols) - set(all_cols)\n",
    "            if missing or extra:\n",
    "                raise ValueError(\n",
    "                    f\"Column mismatch in {fn}.\\nMissing: {missing}\\nExtra: {extra}\"\n",
    "                )\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    if all_cols is None:\n",
    "        raise ValueError(\"No input files were found/loaded.\")\n",
    "\n",
    "    return all_rows, all_cols\n",
    "\n",
    "def build_Xy(rows, target_col, leakage_cols, normalize_target=True):\n",
    "    \"\"\"\n",
    "    Build X,y from dict rows:\n",
    "      - target is Int(G) (raw) or Int(G)/C(n,2) if normalize_target\n",
    "      - features exclude leakage cols and 'id' (position is index)\n",
    "      - we include 'n_from_filename' as a feature (important for combined analysis)\n",
    "    \"\"\"\n",
    "    drop_cols = {\"id\"}  # drop id if present\n",
    "    leakage_cols = set(leakage_cols)\n",
    "\n",
    "    # Determine available columns from first row\n",
    "    cols = list(rows[0].keys())\n",
    "    if target_col not in cols:\n",
    "        raise KeyError(f\"Target column '{target_col}' not found. Available: {cols}\")\n",
    "\n",
    "    # Feature set\n",
    "    feature_names = []\n",
    "    for c in cols:\n",
    "        if c == target_col:\n",
    "            continue\n",
    "        if c in drop_cols:\n",
    "            continue\n",
    "        if c in leakage_cols:\n",
    "            continue\n",
    "        # Always keep n_from_filename\n",
    "        feature_names.append(c)\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for r in rows:\n",
    "        n_val = int(r[\"n_from_filename\"])\n",
    "        # target\n",
    "        t = float(r[target_col])\n",
    "        if normalize_target:\n",
    "            t = t / float(choose2(n_val))\n",
    "        y.append(t)\n",
    "\n",
    "        # features\n",
    "        feat = []\n",
    "        for c in feature_names:\n",
    "            v = r[c]\n",
    "            if v in (\"True\", \"False\"):\n",
    "                feat.append(1.0 if v == \"True\" else 0.0)\n",
    "            else:\n",
    "                if v in (\"+Infinity\", \"Infinity\", \"inf\", \"Inf\", \"nan\", \"NaN\", \"\"):\n",
    "                    raise ValueError(f\"Non-finite value in column '{c}': {v}\")\n",
    "                feat.append(float(v))\n",
    "        X.append(feat)\n",
    "\n",
    "    X = np.array(X, dtype=float)\n",
    "    y = np.array(y, dtype=float)\n",
    "\n",
    "    return X, y, feature_names\n",
    "\n",
    "def print_top_importances(feature_names, values, std=None, topk=15, title=\"\"):\n",
    "    order = np.argsort(values)[::-1]\n",
    "    if title:\n",
    "        print(title)\n",
    "    for k in order[:topk]:\n",
    "        if std is None:\n",
    "            print(f\"  {feature_names[k]:30s} {values[k]:.6f}\")\n",
    "        else:\n",
    "            print(f\"  {feature_names[k]:30s} {values[k]:.6f} +/- {std[k]:.6f}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main: load, split, train, report\n",
    "# -----------------------------\n",
    "rows, cols = load_combined_data(nMin, nMax)\n",
    "X, y, feature_names = build_Xy(\n",
    "    rows,\n",
    "    target_col=TARGET_COL,\n",
    "    leakage_cols=LEAKAGE_COLS,\n",
    "    normalize_target=NORMALIZE_TARGET\n",
    ")\n",
    "\n",
    "# Choose split\n",
    "rs = int(RANDOM_STATE)\n",
    "\n",
    "if TEST_ON_UNSEEN_N:\n",
    "    # Hold out all graphs with n_from_filename == HELD_OUT_N\n",
    "    n_feat_idx = feature_names.index(\"n_from_filename\")\n",
    "    n_values = X[:, n_feat_idx].astype(int)\n",
    "\n",
    "    test_mask = (n_values == int(HELD_OUT_N))\n",
    "    train_mask = ~test_mask\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test   = X[test_mask],  y[test_mask]\n",
    "\n",
    "    print(f\"\\n=== Split: train on n in [{nMin},{nMax}]\\\\{{{HELD_OUT_N}}}, test on n={HELD_OUT_N} ===\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=rs\n",
    "    )\n",
    "    print(\"\\n=== Split: random 75/25 ===\")\n",
    "\n",
    "print(f\"Samples total: {len(y)} | train: {len(y_train)} | test: {len(y_test)}\")\n",
    "print(f\"Features used: {len(feature_names)}\")\n",
    "print(f\"Target: {TARGET_COL} {'(normalized by C(n,2))' if NORMALIZE_TARGET else '(raw)'}\")\n",
    "print(f\"Leakage removed from features: {list(LEAKAGE_COLS)}\\n\")\n",
    "\n",
    "# Train RF (safe settings: n_jobs=1)\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=int(N_ESTIMATORS),\n",
    "    random_state=rs,\n",
    "    n_jobs=1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "r2_train = rf.score(X_train, y_train)\n",
    "r2_test  = rf.score(X_test, y_test)\n",
    "\n",
    "print(\"=== Random Forest (combined n, leakage removed) ===\")\n",
    "print(f\"R^2 train: {r2_train:.4f}\")\n",
    "print(f\"R^2 test : {r2_test:.4f}\\n\")\n",
    "\n",
    "# Importances\n",
    "imp = rf.feature_importances_\n",
    "print_top_importances(feature_names, imp, title=\"Top features (impurity importance):\")\n",
    "\n",
    "perm = permutation_importance(\n",
    "    rf, X_test, y_test,\n",
    "    n_repeats=int(N_REPEATS_PI),\n",
    "    random_state=rs,\n",
    "    n_jobs=1\n",
    ")\n",
    "print()\n",
    "print_top_importances(feature_names, perm.importances_mean, perm.importances_std,\n",
    "                      title=\"Top features (permutation importance on test set):\")\n",
    "\n",
    "# Optional: quick sanity check about the n feature\n",
    "if \"n_from_filename\" in feature_names:\n",
    "    i = feature_names.index(\"n_from_filename\")\n",
    "    print(f\"\\nPermutation importance of n_from_filename: {perm.importances_mean[i]:.6f} +/- {perm.importances_std[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "72b923",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "sage-10.7",
    "--python",
    "-m",
    "sage.repl.ipython_kernel",
    "--matplotlib=inline",
    "-f",
    "{connection_file}"
   ],
   "display_name": "SageMath 10.7",
   "env": {
   },
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 10,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-10.7",
   "resource_dir": "/ext/jupyter/kernels/sage-10.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}